<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Timoka&#39;s Blog</title>
    <link>https://Timoka47.github.io/cn/posts/</link>
    <description>Recent content in Posts on Timoka&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://Timoka47.github.io/cn/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CaiT</title>
      <link>https://Timoka47.github.io/cn/posts/cait/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://Timoka47.github.io/cn/posts/cait/</guid>
      <description>Going deeper with Image Transformers
Abstract  目前对图像transformer的优化研究工作还很少，在本文中，我们搭建了更深层次的transformer用于图像分类，提出了两个架构变化，显著提高了transformer的深度和精度。这使得我们的模型性能不会随着更多的深度而早期饱和。
Introduction  首先，残差结构可以定义为： $$ x_{l+1}=g_{l}(x_{l})+R_{l}(x_{l}) $$ 其中，$g_{l}$通常是恒等映射，$R_{l}$是block的主要操作。残差结构突出了体系结构设计和提高精度之间的强大相互作用。在ResNet论文中提到说，残差网络并不能提供更好的表示能力，但效果更好的原因是网络更容易训练。
Transformer架构可以表示为： $$ x_{l}^{&#39;}=x_{l}+SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\eta$是指LN。上述结构遵循残差连接。
如何归一化、初始化权重或初始化残差块是值得思考的，我们共讨论了4个方法，分别是：Fixup、T-Fixup、ReZero和SkipInit。
在分析了不同初始化、优化和架构设计之间的相互作用之后，我们提出了一种方法，与目前的图像变压器方法相比，它可以有效地改进更深层次的架构训练。具体的，在每个残差块的输出上添加一个可学习的对角矩阵，初始化接近为0，但不是0。我们称之为LayerScale。
其次，我们将patch之间的注意力transformer层和将patch内容传递到单个向量中的层分离，以便单个向量进行分类。这种显式的分离避免了在处理class embedding时引导注意力过程的矛盾目标。我们称之为CaiT（Class-Attention in Image Transformers）。
Deeper image transformers with LayerScale  在ViT和DeiT的工作中，没有证据表明深度更有效：更深的ViT架构性能较低，而DeiT只考虑12层的变压器。
下图为我们比较的几个变体： （a）原始的结构：ViT和DeiT中使用的，即pre-norm结构，层归一化在残差分支的开始部分，但在原始的attention文章中（Attention is all you need）是post-norm，但在我们的实验中，post-norm并不能收敛。
（b）ReZero/Skipinit/Fixup：在残差块的输出部分引入了可学习的标量权重$\alpha _ {l}$，同时出去了pre-norm，可以表示为： $$ x_{l}^{&#39;}=x_{l}+\alpha _ {l} SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+\alpha _ {l}^{&#39;} FFN(\eta (x_{l}^{&#39;})) $$ ReZero是将该参数初始化为0；Fixup将其初始化为1，并进行了其他的修改；但在我们的实验中，这些都没有收敛。
（c）我们的经验观察是，消除pre-norm是使在Fixup中的训练不稳定的原因，因此我们重新引入，从之后的实验中可以看到，是有效的。
（d）Our proposal LayerScale：残差块的输出进行每个通道的相乘，而不是单个标量。即对Self-attention或者FFN的输出乘以一个对角矩阵，目的是给不同的channel乘以不同的$\lambda $值，同时保持Layer Normalization： $$ x_{l}^{&#39;}=x_{l}+diag(\lambda _ {l,1},&amp;hellip;,\lambda _ {l,d}) SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+diag({\lambda }&#39;_ {l,1},&amp;hellip;,{\lambda }&#39;_ {l,d}) FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\lambda _ {l,1}$和${\lambda }&#39;_ {l,1}$都是可学习的参数权重。</description>
    </item>
    
    <item>
      <title>赤壁赋</title>
      <link>https://Timoka47.github.io/cn/posts/cn/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://Timoka47.github.io/cn/posts/cn/</guid>
      <description>原文 壬戌之秋，七月既望，蘇子與客泛舟遊於赤壁之下。清風徐來，水波不興。舉酒屬客，誦明月之詩，歌窈窕之章。少焉，月出於東山之上，徘徊於斗牛之間。白露橫江，水光接天。縱一葦之所如，凌萬頃之茫然。浩浩乎如馮虛御風，而不知其所止；飄飄乎如遺世獨立,羽化而登仙。
於是飲酒樂甚，扣舷而歌之。歌曰：“桂棹兮蘭槳，擊空明兮溯流光。渺渺兮予懷，望美人兮天一方。”客有吹洞簫者，倚歌而和之。其聲嗚嗚然，如怨如慕，如泣如訴；餘音嫋嫋，不絕如縷。舞幽壑之潛蛟，泣孤舟之嫠婦。
蘇子愀然，正襟危坐，而問客曰：“何爲其然也？”客曰：“‘月明星稀，烏鵲南飛。’此非曹孟德之詩乎？西望夏口，東望武昌，山川相繆，鬱乎蒼蒼，此非孟德之困於周郎者乎？方其破荊州，下江陵，順流而東也，舳艫千里，旌旗蔽空，釃酒臨江，橫槊賦詩，固一世之雄也，而今安在哉？況吾與子漁樵於江渚之上，侶魚蝦而友麋鹿，駕一葉之扁舟，舉匏樽以相屬。寄蜉蝣於天地，渺滄海之一粟。哀吾生之須臾，羨長 江之無窮。挾飛仙以遨遊，抱明月而長終。知不可乎驟得，託遺響於悲風。”
蘇子曰：“客亦知夫水與月乎？逝者如斯，而未嘗往也；盈虛者如彼，而卒莫消長也。蓋將自其變者而觀之，則天地曾不能以一瞬；自其不變者而觀之，則物與我皆無盡也，而又何羨乎！且夫天地之間，物各有主，苟非吾之所有，雖一毫而莫取。惟江上之清風，與山間之明月，耳得之而爲聲，目遇之而成色，取之無禁，用之不竭。是造物者之無盡藏也，而吾與子之所共適。”(共適 一作：共食) 客喜而笑，洗盞更酌。餚核既盡，杯盤狼籍。相與枕藉乎舟中，不知東方之既白。</description>
    </item>
    
  </channel>
</rss>
