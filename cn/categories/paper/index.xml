<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on Timoka&#39;s Blog</title>
    <link>https://Timoka47.github.io/cn/categories/paper/</link>
    <description>Recent content in Paper on Timoka&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://Timoka47.github.io/cn/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CaiT</title>
      <link>https://Timoka47.github.io/cn/posts/cait/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://Timoka47.github.io/cn/posts/cait/</guid>
      <description>Going deeper with Image Transformers
Abstract  目前对图像transformer的优化研究工作还很少，在本文中，我们搭建了更深层次的transformer用于图像分类，提出了两个架构变化，显著提高了transformer的深度和精度。这使得我们的模型性能不会随着更多的深度而早期饱和。
Introduction  首先，残差结构可以定义为： $$ x_{l+1}=g_{l}(x_{l})+R_{l}(x_{l}) $$ 其中，$g_{l}$通常是恒等映射，$R_{l}$是block的主要操作。残差结构突出了体系结构设计和提高精度之间的强大相互作用。在ResNet论文中提到说，残差网络并不能提供更好的表示能力，但效果更好的原因是网络更容易训练。
Transformer架构可以表示为： $$ x_{l}^{&#39;}=x_{l}+SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\eta$是指LN。上述结构遵循残差连接。
如何归一化、初始化权重或初始化残差块是值得思考的，我们共讨论了4个方法，分别是：Fixup、T-Fixup、ReZero和SkipInit。
在分析了不同初始化、优化和架构设计之间的相互作用之后，我们提出了一种方法，与目前的图像变压器方法相比，它可以有效地改进更深层次的架构训练。具体的，在每个残差块的输出上添加一个可学习的对角矩阵，初始化接近为0，但不是0。我们称之为LayerScale。
其次，我们将patch之间的注意力transformer层和将patch内容传递到单个向量中的层分离，以便单个向量进行分类。这种显式的分离避免了在处理class embedding时引导注意力过程的矛盾目标。我们称之为CaiT（Class-Attention in Image Transformers）。
Deeper image transformers with LayerScale  在ViT和DeiT的工作中，没有证据表明深度更有效：更深的ViT架构性能较低，而DeiT只考虑12层的变压器。
下图为我们比较的几个变体： （a）原始的结构：ViT和DeiT中使用的，即pre-norm结构，层归一化在残差分支的开始部分，但在原始的attention文章中（Attention is all you need）是post-norm，但在我们的实验中，post-norm并不能收敛。
（b）ReZero/Skipinit/Fixup：在残差块的输出部分引入了可学习的标量权重$\alpha _ {l}$，同时出去了pre-norm，可以表示为： $$ x_{l}^{&#39;}=x_{l}+\alpha _ {l} SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+\alpha _ {l}^{&#39;} FFN(\eta (x_{l}^{&#39;})) $$ ReZero是将该参数初始化为0；Fixup将其初始化为1，并进行了其他的修改；但在我们的实验中，这些都没有收敛。
（c）我们的经验观察是，消除pre-norm是使在Fixup中的训练不稳定的原因，因此我们重新引入，从之后的实验中可以看到，是有效的。
（d）Our proposal LayerScale：残差块的输出进行每个通道的相乘，而不是单个标量。即对Self-attention或者FFN的输出乘以一个对角矩阵，目的是给不同的channel乘以不同的$\lambda $值，同时保持Layer Normalization： $$ x_{l}^{&#39;}=x_{l}+diag(\lambda _ {l,1},&amp;hellip;,\lambda _ {l,d}) SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+diag({\lambda }&#39;_ {l,1},&amp;hellip;,{\lambda }&#39;_ {l,d}) FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\lambda _ {l,1}$和${\lambda }&#39;_ {l,1}$都是可学习的参数权重。</description>
    </item>
    
  </channel>
</rss>
